{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFrekans tablosu oluşturan betil\\n\\nTrain dosyalarını okur. Zemberek ile normalleştirir ve ayıklar. \\nDaha sonra ayıklanmış kelimeleri köklerine ayırır.\\nBu kökler etkisiz kelimler ise (stop words) onları siler.\\nBu kökler yardımıyla eşsiz kelimeleri bulur.\\nBu eşsiz kelimelerin dökü\\nmanlarda geçme sıklıklarına göre frekans tablosu oluşturur.\\nBu tabloyu tf.csv dosyasına yazar.\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Frekans tablosu oluşturan betil\n",
    "\n",
    "Train dosyalarını okur. Zemberek ile normalleştirir ve ayıklar. \n",
    "Daha sonra ayıklanmış kelimeleri köklerine ayırır.\n",
    "Bu kökler etkisiz kelimler ise (stop words) onları siler.\n",
    "Bu kökler yardımıyla eşsiz kelimeleri bulur.\n",
    "Bu eşsiz kelimelerin dökü\n",
    "manlarda geçme sıklıklarına göre frekans tablosu oluşturur.\n",
    "Bu tabloyu tf.csv dosyasına yazar.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import sys\n",
    "import zemberek_grpc.language_id_pb2 as z_langid\n",
    "import zemberek_grpc.language_id_pb2_grpc as z_langid_g\n",
    "import zemberek_grpc.normalization_pb2 as z_normalization\n",
    "import zemberek_grpc.normalization_pb2_grpc as z_normalization_g\n",
    "import zemberek_grpc.preprocess_pb2 as z_preprocess\n",
    "import zemberek_grpc.preprocess_pb2_grpc as z_preprocess_g\n",
    "import zemberek_grpc.morphology_pb2 as z_morphology\n",
    "import zemberek_grpc.morphology_pb2_grpc as z_morphology_g\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from threading import Thread\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start zemberek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define base zemberek functions\n",
    "channel = grpc.insecure_channel('localhost:1234')\n",
    "\n",
    "langid_stub = z_langid_g.LanguageIdServiceStub(channel)\n",
    "normalization_stub = z_normalization_g.NormalizationServiceStub(channel)\n",
    "preprocess_stub = z_preprocess_g.PreprocessingServiceStub(channel)\n",
    "morphology_stub = z_morphology_g.MorphologyServiceStub(channel)\n",
    "\n",
    "def find_lang_id(i):\n",
    "    response = langid_stub.Detect(z_langid.LanguageIdRequest(input=i))\n",
    "    return response.langId\n",
    "\n",
    "def tokenize(i):\n",
    "    response = preprocess_stub.Tokenize(z_preprocess.TokenizationRequest(input=i))\n",
    "    return response.tokens\n",
    "\n",
    "def normalize(i):\n",
    "    response = normalization_stub.Normalize(z_normalization.NormalizationRequest(input=i))\n",
    "    return response\n",
    "\n",
    "def analyze(i):\n",
    "    response = morphology_stub.AnalyzeSentence(z_morphology.SentenceAnalysisRequest(input=i))\n",
    "    return response;\n",
    "\n",
    "def fix_decode(text):\n",
    "    \"\"\"Pass decode.\"\"\"\n",
    "    if sys.version_info < (3, 0):\n",
    "        return text.decode('utf-8')\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(map(lambda x: x.replace(\"\\n\",\"\").encode(\"utf-8\"), open(\"stop-words-2.txt\",encoding=\"iso-8859-9\").readlines()))\n",
    "def isStopWord(word):\n",
    "    if word == \"UNK\":\n",
    "        return True\n",
    "    if len(word) <= 2:\n",
    "        return True\n",
    "    return word in stop_words\n",
    "\n",
    "def preprocess(document):\n",
    "    tokenized = fix_decode(tokenize(normalize(document).normalized_input))\n",
    "    output = []\n",
    "    for i in tokenized:\n",
    "        if i.type == 'Word':\n",
    "            lemma = analyze(i.token).results[0].best.lemmas[0]\n",
    "            #print(\"lemma(%s)=%s\"%(i.token,lemma))\n",
    "            if not isStopWord(lemma):\n",
    "                output.append(str(lemma))\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessWrite(path):\n",
    "    f_in = open(path,encoding=\"iso-8859-9\").read()\n",
    "    output = preprocess(f_in)\n",
    "    f_name = path.split(\"/\")[-1]\n",
    "    f_out_path = path.replace(f_name,\"\")+\"../../../data-preprocessed\"+path.replace(\"../data\",\"\")\n",
    "    f_out = open(f_out_path,\"wb\")\n",
    "    f_out.write(json.dumps(output,ensure_ascii=False).encode(\"utf-8\"))\n",
    "    f_out.close()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../data/train\"\n",
    "ignore = ['.DS_Store']\n",
    "folders = set(os.listdir(root_path))-set(ignore)\n",
    "txt_files = []\n",
    "for folder in folders:\n",
    "    folder_path = \"%s/%s/\" % (root_path,folder)\n",
    "    for file in set(os.listdir(folder_path))-set(ignore):\n",
    "        file_path = \"%s/%s/%s\" %(root_path,folder,file)\n",
    "        txt_files.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "for file in txt_files:\n",
    "    outputs = preprocessWrite(file)\n",
    "    for i in outputs:\n",
    "        words.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(frozenset(words))\n",
    "words.append('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "file_names = []\n",
    "for k in range(len(txt_files)):\n",
    "    txt_file = txt_files[k].replace(\"data\",\"data-preprocessed\")\n",
    "    doc_type = txt_file.split(\"/\")[3]\n",
    "    file_names.append(txt_files[k].split(\"/\")[-1])\n",
    "    row = [0]*(len(words))\n",
    "    document_words = json.loads(open(txt_file, encoding='utf-8').read())\n",
    "    for index,word in enumerate(words):\n",
    "        row[index] = document_words.count(word)\n",
    "    row[-1] = doc_type\n",
    "    row = tuple(row)\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(rows,columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.insert(0,'files',file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(\"../tf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
